---
title: "Understanding Bayesian Inference"
author: "Zach Smith"
output:
  html_document:
    df_print: paged
---

This notebook was developed to provide an intuitive understanding of different bayesian inference techniques. The focus is on the what and how and not on the why. Mathematical derivations for *why* a technique works are left for further research, instead I hope to demonstrate what each inference technique aims to accomplish and show how to accomplish this by implementing simple solutions with minimal tooling to make the mechanics visible and easily understood. I also hope to show how each technique relates to the other to help provide a unified picture of what we are trying to accomplish in bayesian inference.

This notebook is composed of two main sections representing two common problems faced in bayesian inference.

1. Parameter inference (global parameters)
2. Latent variable inference (local parameters)

```{r setup, echo = FALSE}
options(tidyverse.quiet = TRUE)
library(tidyverse)
```


Suppose have a random variable $X$ that is distributed

$$
\mu = 3 \\
\sigma = 10 \\
X \sim Normal(\mu, \sigma )
$$

```{r generate data}
set.seed(1)

N <- 60
mu <- 3
sigma <- 10
x <- rnorm(N, mu, sigma)

hist(x)
```

If $\mu$ is unknown how do we perform inference? In bayesian inference we hope to find the posterior of a parameter given some data $p(\mu|X=x)$. We can accomplis this using bayes rule:

$$
p(\mu|x) = \frac{p(x|\mu)p(\mu)}{\int_{\mu} p(x,\mu)d\mu} = \frac{p(x|\mu)p(\mu)}{p(x)}
$$

By specifying a prior over $\mu$ we can then update this distribution given some data to produce a posterior distribution. Three common methods for solving this problem follow.

1. [Conjugate Prior](#conjugate-prior)
2. [MCMC]($mcmc)
3. [Variational Inference](#variational-inference)

# Conjugate Prior

By matching the likelihood distribution with a conjugate prior we can solve for $p(x)$ analytically and recover a posterior distribution whose parameters can be estimated analytically as well.

A normal distribution is the conjugate prior for $\mu$ when the data likelihood is also a normal distribution and the standard deviation is known.

$$
\mu \sim Normal(\mu_\mu, \sigma_\mu)
$$

The parameter estimates for the posterior distribution of $\mu$ are:

$$
\mu_{\mu|x} = \frac{\frac{\mu_\mu}{\sigma_\mu^2} + \frac{\sum X}{\sigma^2}}{1/\sigma_\mu^2 + N/\sigma^2} \\
\sigma_{\mu|x} = \sqrt{\frac{1}{\frac{1}{\sigma_mu^2} + \frac{N}{\sigma^2}}} 
$$
In this case lets set the hyperparameters for $\mu$ as:


$$
\mu_\mu = 2 \\
\sigma_\mu = 2 \\
\mu \sim N(\mu_\mu, \sigma_\mu) \\
\sigma = 10 \\
X \sim N(\mu, \sigma )
$$

```{r}
prior_mu <- 2
prior_sigma <- 2

mu_mu_conj <- (prior_mu / prior_sigma^2 + sum(x) / sigma^2) / (1 / prior_sigma^2 + N / sigma^2) # posterior mean for mu
mu_sigma_conj <- sqrt(1 / (1 / prior_sigma^2 + N / sigma^2)) # posterior sd for mu

estimates <- data.frame(method = "Conjugate", mu_mu = mu_mu_conj, mu_sigma = mu_sigma_conj)
knitr::kable(estimates)
```

This gives us an estimate of the mean of $\mu$ of about 3.47 with a standard deviation of about 1.08, so our posterior $p(\mu|x)$ is distributed:

$$
\mu_{\mu|x} = 3.47 \\
\sigma_{\mu|x} = 1.08 \\
\mu \sim Normal(\mu_{\mu|x}, \sigma_{\mu|x})
$$

# MCMC

In some cases we may choose a prior for $/mu$ which we are unable or unwilling to solve for $p(x)$ analytically. In that case we cannot analytically recover the posterior $p(\mu|x)$, however, using Markov Chain Monte Carlo we can sample from the posterior.

If we use the same prior distribution and parameters as before $\mu_\mu=2,\sigma_\mu=2$,  that means we wish to draw samples that are normally distributed with mean 3.47 and standard deviation 1.08 without actually knowing those parameters.

There are different methods for performing this sampling (Gibbs, Hamiltonian, Metropolis Hastings), but the basic idea behind all of them is that we generate a series of random samples (Monte Carlo), with each sample being dependent on the previous one (Markov Chain). By being particular about which of the samples in this series we keep we can shape an arbitrary proposal distribution into producing samples that match our posterior distribution.

For this we will use one of the simplest MCMC sampling methods, Metropolis Hastings. For this there are two things we need.

1. A proposal distribution. This is an arbitrary distribution which we use to draw samples from.

For this we will use a normal distribution centered at the previous sample location (Markov Chain) with a standard deviation of 1.

$$
\mu_1 \sim Normal(\mu_0, 1)
$$
```{r proposal distribution}
# we need a proposal distribution 
# very tiny jumps == high acceptance rate but slow mixing
jump_sigma <- 1
rproposal <- function(mu_) rnorm(1, mu_, jump_sigma) 
```

The proposal distribution can be any arbitrary distribution we want. The choice of a symmetric distribution where $p(\mu_1|\mu_0)=p(\mu_0|\mu_1)$ simplifies the acceptance rule for us. The choice the standard deviation of 1 is also fairly arbitrary, in theory it will not affect the results, but in practice it influences how quickly the markov chain explores the posterior (larger sd will jump to farther away locations in the posterior) as well as our overall acceptance probabilities (larger sd will result in lower acceptance probabilities) which both influence how long we need to run our chain.

2. An acceptance rule. This is a rule dependent on the previous sample and current sample to determine whether or not to keep the current sample.

For Metropolis Hastings with a symettric proposal distribution this rule is the ratio of the posterior likelihood given the proposed $\mu$ over the posterior likelihood given the previous $\mu$

$$
p(Accept) =  \frac{p(\mu_1|x)}{p(\mu_0|x)} = \frac{p(x|\mu_1)p(\mu_1|\mu_\mu,\sigma_\mu)}{p(x|\mu_0)p(\mu_0|\mu_\mu,\sigma_\mu)}
$$

```{r acceptance probaility}
pacceptance <- function(mu1, mu0) {
  # calculations done in log for numerical stability (prevent underflows)
  log_likelihood1 <- dnorm(mu1, prior_mu, prior_sigma, log = TRUE) + sum(dnorm(x, mu1, sigma, log = TRUE))
  log_likelihood0 <- dnorm(mu0, prior_mu, prior_sigma, log = TRUE) + sum(dnorm(x, mu0, sigma, log = TRUE))
  exp(log_likelihood1 - log_likelihood0)
}
```


Now that we have a proposal distribution and an acceptance rule we can run our MCMC algorithm.

```{r mcmc inference}
mu0 <- rnorm(1, prior_mu, prior_sigma) #select initial value from the prior (can be done other ways)
mus <- c() # collect the markov chain values here
# run Metropolis Hastings MCMC
iters <- 10000
for(i in 1:iters){
  mu1 <- rproposal(mu0) # propose new mu based on current mu
  acceptance_ratio <- pacceptance(mu1, mu0) # calculate acceptance probability
  # accept or reject with probability equal to acceptance_ratio
  u <- runif(1)
  mu0 <- ifelse(u <= acceptance_ratio, mu1, mu0) # if accept current mu it becomes previous mu for next step
  mus <- c(mus, mu0) # add current sample value to chain
}
```

By calculating summary statistics on the sampled values we can see if they match the analytical solution.

```{r mcmc posterior}
mu_mu_mcmc <- mean(mus)
mu_sigma_mcmc <- sd(mus)

estimates <- rbind(estimates, data.frame(method = "MCMC", mu_mu = mu_mu_mcmc, mu_sigma = mu_sigma_mcmc))
knitr::kable(estimates)
```

We see they are fairly close, showing that we were able to sample from the posterior distribution. Lets plot the sampled distribution against the true posterior.

```{r plot mcmc posterior}
x1 <- seq(-10, 20, by = 0.1)
hist(mus, freq = FALSE, main = "Histogram of MCMC Draws")
lines(y = dnorm(x1, mu_mu_conj, mu_sigma_conj ), x = x1)
legend("topleft", legend=c("Conjugate Posterior"), lty=c(1))
```

# Variational Inference

Perhaps we are impatient with MCMC so we would like to turn this into an optimization problem. We can do this using variational inference. In this scenario we will specify an approximating posterior distribution $q(\mu|x)$ along with the prior distribution. This posterior distribution can be any distribution we like but our goal is the select a distribution that can match the true posterior as closely as possible. We then optimize the parameters of the approximate posterior to do this.

Because we are using conjugate priors we actually know that the true posterior is normally distributed, so first let's pick the normal as our approximate distribution and see if we can properly recover the parameters. Next, we will try using a distribution that cannot so closely approximate the true normal posterior and see what happens.

With a normal distribution as our posterior we have two parameters to optimize for it, $\mu_q$ and $\sigma_q$.

$$
q(\mu|x) \sim Normal(\mu_q, \sigma_q)
$$

```{r approximate posterior}
q_z <- rnorm
```

In order for us to optimize these parameters we need to define our loss function. This is normally taken to be the KL divergence between the approximate posterior and the true posterior. With some work the KL divergence can be simplified to a form which we can actually calculate, in practice this is generally converted to the ELBO to be maximized. The details of this is left to the reader to explore further.

$$
KL(q(\mu|x)||p(\mu|x)) = \int_\mu q(\mu|x) \log\frac{q(\mu|x)}{p(\mu|x)}d\mu = \\
E[\log\frac{q(\mu|x)}{p(\mu|x)}] = \\
E[\log q(\mu|x) - \log p(\mu, x) + \log p(x)]  = \\
E[\log q(\mu|x) - \log p(x|\mu) + \log p(\mu) + \log p(x)]
$$

The last term $\log p(x)$ does not rely on $q(\mu|x)$ and is therefore constant within the optimization problem reducing the KL divergence to_search

$$
E[\log q(\mu|x) - \log p(x|\mu) + \log p(\mu)]
$$

Because the result is an expectation, we can calculate this by Monte Carlo sampling the approximate posterior and take the average of the resulting calculations.

```{r kl divergence}
n_draws = 100 #number of draws for the expectation in KL divergence

kl_divergence <- function(posterior_mu, posterior_sigma){
  #take samples from approximate posterior distribution
  z <- q_z(n_draws, posterior_mu, posterior_sigma)
  #calculate elbo
  kl <- sapply(z, function(z_i){
    dnorm(z_i, posterior_mu, posterior_sigma, log = TRUE) - sum(dnorm(x, z_i, sigma, log = TRUE)) - dnorm(z_i, prior_mu, prior_sigma, log = TRUE)
  })
  mean(kl) #take expectation
}
```

All that is left is to optimize the `posterior_mu` and `posterior_sigma`. Some sort of gradient descent is generally used, but lets to a grid search over the parameter space so we can look at how kl divergence changes across this space. We will calculate the KL divergence at each combination of proposed `posterior_mu` and `posterior_sigma`, select the combination that produces the lowest value, and plot it on the space of values.

```{r vi grid search}
# grid search over all combinations of mu_vals and sigma_vals
mu_vals <- seq(-2, 8, by = 0.1)
sigma_vals <- seq(0.1, 10, by = 0.1)
to_search <- expand.grid(mu_vals, sigma_vals)
to_search$kl <- 0
for(i in 1:nrow(to_search)){
  to_search[i, "kl"] <- kl_divergence(to_search[i,1], to_search[i,2])
}
to_search <- setNames(to_search, c("mu_mu", "mu_sigma", "kl"))

best_val <- to_search[which.min(to_search$kl),]
estimates <- rbind(estimates, data.frame(method = "VI-Grid Search", mu_mu = best_val$mu_mu, mu_sigma = best_val$mu_sigma))
knitr::kable(estimates)
```

From our grid search we see that we have once again come very close to the true posterior values.

```{r plot vi grid}
to_search %>% 
  ggplot(aes(x = mu_mu, y = mu_sigma, fill =kl)) +
  geom_tile() +
  geom_point(data = best_val, aes(x = mu_mu, y = mu_sigma), col="red")
```

This time, lets perform gradient descent. We

```{r vi coordinate ascent}
# coordinate descent
mu_mu_kl <- 0
mu_sigma_kl <- 5

iters <- 10000
h <- 1e-2
step_size <- 1e-3
steps <- as.data.frame(matrix(0, nrow = iters, ncol = 2)) %>%  setNames(c("mu", "sigma"))
for(i in 1:iters){
  #step mu
  gradient <- (kl_divergence(mu_mu_kl + h, mu_sigma_kl) - kl_divergence(mu_mu_kl, mu_sigma_kl)) / h
  mu_mu_kl <- mu_mu_kl - gradient * step_size
  #step sigma
  gradient <- (kl_divergence(mu_mu_kl, mu_sigma_kl + h) - kl_divergence(mu_mu_kl, mu_sigma_kl)) / h
  mu_sigma_kl <- mu_sigma_kl - gradient * step_size
  
  steps[i,] <- c(mu_mu_kl, mu_sigma_kl)
}
```

```{r}
estimates <- rbind(estimates, data.frame(method = "VI-Gradient Descent", mu_mu = mu_mu_kl, mu_sigma = mu_sigma_kl))
knitr::kable(estimates)
```


```{r plot vi grid posterior}
x1 <- seq(-20, 20, by=0.01) + mu
plot(x = x1, y = dnorm(x1, mu_mu_kl, mu_sigma_kl), type="l")
abline(v = mu)
abline(v = mu_mu_kl, lty=2)
legend("topleft", legend=c("mu", "mu_hat"), lty=c(1,2))
```

```{r plot vi coordinate ascent}
to_search %>% 
  ggplot() +
  geom_tile(aes(x = mu_mu, y = mu_sigma, fill = kl)) +
  geom_path(data = steps, aes(x = mu, y = sigma), col="red")
```

# Amortized Variational Inference

```{r gen latent variable data}
N <- 60
beta <- 2
sigma <- 1
x <- rnorm(N)
y <- rnorm(N, x * beta, sigma)
```


```{r plot latent and observed}
plot(y~x)
```

```{r amortized vi}
prior_mu <- 0
prior_sigma <- 1
ndraws <- 100

normal_network <- function(x, params){
  z <- x %*% params
  z[,2] <- exp(z[,2])
  z
}

kl_divergence <- function(encoder_params, decoder_params){
  posterior_vals <- normal_network(y, encoder_params)
  #calculate elbo
  kl <- sapply(1:ndraws, function(i){
    #take samples from approximate distribution
    z <- rnorm(length(x), posterior_vals[,1], posterior_vals[,2])
    y_params <- normal_network(z, decoder_params)
    kl_ <- sum(dnorm(z, posterior_vals[,1], posterior_vals[,2], log = TRUE)) - 
      sum(dnorm(y, y_params[,1], y_params[,2], log = TRUE)) - 
      sum(dnorm(z, prior_mu, prior_sigma, log = TRUE))
    kl_
  })
  mean(kl)
}

# coordinate ascent
encoder_params <- matrix(rnorm(2, 0, 0.1), nrow = 1)
decoder_params <- matrix(rnorm(2, 0, 0.1), nrow = 1)

iters <- 100
h <- 1e-2
h1 <- c(h, 0)
h2 <- c(0, h)
step_size <- 1e-1
encoder_steps <- as.data.frame(matrix(0, nrow = iters, ncol = 2)) %>%  setNames(c("b0", "b1"))
decoder_steps <- as.data.frame(matrix(0, nrow = iters, ncol = 2)) %>%  setNames(c("b0", "b1"))
x_steps <- as.data.frame(matrix(0, nrow = iters, ncol = N))
kls <- c()
for(i in 1:iters){
  #optimize encoder
  #step encoder b1
  gradient <- (kl_divergence(encoder_params + h1, decoder_params) - kl_divergence(encoder_params, decoder_params)) / h
  encoder_params <- encoder_params - h1 * gradient * step_size
  #step encoder b2
  gradient <- (kl_divergence(encoder_params + h2, decoder_params) - kl_divergence(encoder_params, decoder_params)) / h
  encoder_params <- encoder_params - h2 * gradient * step_size
  #optimize decoder
  #step decoder b1
  gradient <- (kl_divergence(encoder_params, decoder_params + h1) - kl_divergence(encoder_params, decoder_params)) / h
  decoder_params <- decoder_params - h1 * gradient * step_size
  #step decoder b2
  gradient <- (kl_divergence(encoder_params, decoder_params + h2) - kl_divergence(encoder_params, decoder_params)) / h
  decoder_params <- decoder_params - h2 * gradient * step_size
  
  encoder_steps[i,] <- encoder_params
  decoder_steps[i,] <- decoder_params
  x_steps[i,] <- normal_network(y, encoder_params)[,1]
  kls <- c(kls, kl_divergence(encoder_params, decoder_params))
}
encoder_params
decoder_params
```

```{r}
plot(kls, type = "l")
```

```{r plot actual vs latent posterior}
plot(normal_network(y, encoder_params)[,1] ~ x)
```

```{r, include = FALSE}
library(gganimate)

steps_df <- x_steps %>%
  mutate(ix = row_number()) %>%
  pivot_longer(-ix, names_to = "dim", values_to = "value") %>%
  mutate(true = rep(x, iters))


glimpse(steps_df)

learning_plot <- ggplot(steps_df) +
  geom_point(aes(x = true, y = value)) +
  transition_manual(ix)

anim_save("latent_learning_continuous.gif", learning_plot)
```

![](latent_learning_continuous.gif)


# Amortized Variational Inference for Categorical Variables

```{r gen latent variable data2}
N_ <- 60
N <- N_ + N_

mu_1 <- 1
mu_2 <- -1
sigma <- 1

y_1 <- rnorm(N_, mu_1, sigma)
y_2 <- rnorm(N_, mu_2, sigma)
y <- c(y_1, y_2)

y_df <- data.frame(class = as.factor(c(rep(1, N_), rep(2, N_))), y = y)
```


```{r plot latent and observed categorical, warning = FALSE}
y_df %>%
  ggplot(aes(x = y, fill = class, group = class))+
  geom_histogram(position = "identity", alpha = 0.5)
```

```{r amortized vi2}
categorical_network <- function(x, params){
  z <- x %*% params
  z <- exp(z) / rowSums(exp(z))
  z
}

normal_network <- function(x, params){
  z <- x %*% params
  z[,2] <- exp(z[,2])
  z
}

prior_prob <- c(0.5, 0.5)
ndraws <- 100

kl_divergence <- function(encoder_params, decoder_params){
  posterior_vals <- categorical_network(y, encoder_params)
  #calculate elbo
  kl <- sapply(1:ndraws, function(i){
    #take samples from approximate distribution
    z <- (runif(length(y)) <= posterior_vals[,1]) + 1 # convert to {1,2}
    z_hot <- matrix(0, nrow = length(y), ncol = 2)
    z_hot[cbind(1:length(y), z)] <- 1
    y_params <- normal_network(z_hot, decoder_params)
    kl_ <- sum(log(posterior_vals[cbind(1:length(y), z)])) - 
      sum(dnorm(y, y_params[,1], y_params[,2], log = TRUE)) - 
      sum(log(prior_prob[z]))
    kl_
  })
  mean(kl)
}

# randomly initialize parameters
encoder_params <- matrix(rnorm(2, 0, 0.01), nrow = 1)
decoder_params <- matrix(rnorm(4, 0, 0.01), nrow = 2)

iters <- 100
h <- 1e-2
h11 <- c(h, 0)
h12 <- c(0, h)
h21 <- matrix(c(h, 0, 0, 0), nrow = 2, byrow = TRUE)
h22 <- matrix(c(0, h, 0, 0), nrow = 2, byrow = TRUE)
h23 <- matrix(c(0, 0, h, 0), nrow = 2, byrow = TRUE)
h24 <- matrix(c(0, 0, 0, h), nrow = 2, byrow = TRUE)
step_size <- 1e-1
encoder_steps <- as.data.frame(matrix(0, nrow = iters, ncol = 2)) %>%  setNames(c("b0", "b1"))
decoder_steps <- as.data.frame(matrix(0, nrow = iters, ncol = 4)) %>%  setNames(c("b0", "b1"))
x_steps <- as.data.frame(matrix(0, nrow = iters, ncol = N))
kls <- c()
for(i in 1:iters){
  #optimize encoder
  #step encoder b1
  gradient <- (kl_divergence(encoder_params + h11, decoder_params) - kl_divergence(encoder_params, decoder_params)) / h
  encoder_params <- encoder_params - h11 * gradient * step_size
  #step encoder b2
  gradient <- (kl_divergence(encoder_params + h12, decoder_params) - kl_divergence(encoder_params, decoder_params)) / h
  encoder_params <- encoder_params - h12 * gradient * step_size
  #optimize decoder
  #step decoder b11
  gradient <- (kl_divergence(encoder_params, decoder_params + h21) - kl_divergence(encoder_params, decoder_params)) / h
  decoder_params <- decoder_params - h21 * gradient * step_size
  #step decoder b12
  gradient <- (kl_divergence(encoder_params, decoder_params + h22) - kl_divergence(encoder_params, decoder_params)) / h
  decoder_params <- decoder_params - h22 * gradient * step_size
  #step decoder b21
  gradient <- (kl_divergence(encoder_params, decoder_params + h23) - kl_divergence(encoder_params, decoder_params)) / h
  decoder_params <- decoder_params - h23 * gradient * step_size
  #step decoder b22
  gradient <- (kl_divergence(encoder_params, decoder_params + h24) - kl_divergence(encoder_params, decoder_params)) / h
  decoder_params <- decoder_params - h24 * gradient * step_size
  
  encoder_steps[i,] <- encoder_params
  decoder_steps[i,] <- decoder_params
  x_steps[i,] <- network(y, encoder_params)[,1]
  kls <- c(kls, kl_divergence(encoder_params, decoder_params))
}
```

```{r}
plot(kls, type = "l")
```

```{r, include = F}
library(gganimate)

steps_df <- x_steps %>%
  mutate(ix = row_number()) %>%
  pivot_longer(-ix, names_to = "dim", values_to = "value") %>%
  mutate(
    class = as.factor(rep(c(rep(1,N_), rep(2,N_)), iters)),
    y = rep(y, iters)
  )

steps_df %>%
  filter(dim=="V1") %>%
  pull(value) %>% plot()

ggplot(steps_df, aes(x=ix, y=value, group=dim)) + 
geom_line()

glimpse(steps_df)

yi <- rnorm(N)
steps_df %>%
  filter(ix==1) %>%
ggplot( aes(y = yi, x = y, col = value,1)) +
  geom_point() +
  facet_grid(class ~ .) 
```

```{r}
yi <- rep(rnorm(N), iters)
learning_plot <- ggplot(steps_df, aes(y = value, x = y, color = class)) +
  geom_point() +
  transition_manual(ix)

anim_save("latent_learning_class.gif", learning_plot)
```

![](latent_learning2.gif)